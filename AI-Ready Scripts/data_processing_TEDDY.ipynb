{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22843646",
   "metadata": {},
   "source": [
    "*__Note:__* Filepaths should be changed as necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5375d74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "from itertools import compress\n",
    "from IPython.display import display\n",
    "from functools import reduce\n",
    "from math import floor\n",
    "from statistics import mode\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "import subprocess\n",
    "import sys\n",
    "import time\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "try:\n",
    "    import fancyimpute\n",
    "except ImportError:\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", 'fancyimpute'])\n",
    "finally:\n",
    "    import fancyimpute\n",
    "from fancyimpute import IterativeImputer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43196034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Merge 48 studies (*.csv) from the /DATA/TEDDY\n",
    "# Input: file_directory = \"/home/NIDDK/DATA/TEDDY/\"\n",
    "# Output: data frame (817 rows stand for 817 participants, *** columns stand for the tranformed columns from 48 studies)\n",
    "\n",
    "#calculate % completeness of column\n",
    "#missing value check\n",
    "def missing_value(val):\n",
    "    if isinstance(val, str):\n",
    "        val = val.lower()\n",
    "        return val != val or val == \"\" or val == \"null\" or val == \"\" or val == \"nan\" or val == \"not reported\"\n",
    "    else:\n",
    "        return val != val or np.isnan(val)\n",
    "\n",
    "def calculate_completeness(value_list):\n",
    "    return(sum([not(missing_value(val)) for val in value_list])/len(value_list))\n",
    "\n",
    "def remove_columns_low_representation(dataframe, threshold):\n",
    "    column_list = list(dataframe.columns)\n",
    "    column_list = column_list[1:len(column_list)]\n",
    "    completeness_list = [calculate_completeness(dataframe[column]) for column in column_list]\n",
    "    completeness_dict = dict(zip(column_list, completeness_list))\n",
    "    for column in column_list:\n",
    "        if completeness_dict[column] < threshold:\n",
    "            column_list.remove(column)\n",
    "    column_list.insert(0, \"MaskID\")\n",
    "    return dataframe.loc[:, column_list]\n",
    "\n",
    "def generate_split_frame(dataframe, key_column, split_column, split_value):\n",
    "    dataframe = dataframe.drop(split_column, axis = 1)\n",
    "    column_list = list(dataframe.columns)\n",
    "    column_list.pop(0)\n",
    "    modified_column_list = [\"observation_\" + str(split_value) + \"_\" + column \\\n",
    "                           for column in column_list]\n",
    "    dataframe.columns = [key_column] + modified_column_list\n",
    "    return dataframe\n",
    "\n",
    "#used to mark the observation # of each record in files which have multiple records per case (different timepoints)\n",
    "def number_of_appearances(element_list):\n",
    "    element_count_list = []\n",
    "    appearance_dict = {}\n",
    "    for element in element_list:\n",
    "        if element in appearance_dict.keys():\n",
    "            count = appearance_dict[element] + 1\n",
    "            appearance_dict[element] = count\n",
    "            element_count_list = element_count_list + [count]\n",
    "        else:\n",
    "            appearance_dict[element] = 1\n",
    "            element_count_list = element_count_list + [1]\n",
    "    return element_count_list\n",
    "\n",
    "#flattens a frame with multiple records per case after sorting by the sort_column, assigning observation #s\n",
    "#and merging the observation # based frames\n",
    "def sort_and_merge_by_observation(dataframe, sort_column = None):\n",
    "    if not sort_column is None:\n",
    "        dataframe.sort_values(sort_column, inplace = True)\n",
    "    dataframe.loc[:,\"to_split\"] = number_of_appearances(dataframe[\"MaskID\"])\n",
    "    max_split = max(dataframe[\"to_split\"])\n",
    "    return reduce(lambda left, right: pd.merge(left, right, on = \"MaskID\",how = \"outer\"), \\\n",
    "                 [generate_split_frame(dataframe[dataframe[\"to_split\"] == i], \\\n",
    "                                      \"MaskID\", \"to_split\", i) for i in range(1, max_split + 1)])\n",
    "\n",
    "def split_and_merge_by_test_type(dataframe):\n",
    "    distinct_test_types = np.unique(dataframe[\"TEST_NAME\"])\n",
    "    return reduce(lambda left, right: pd.merge(left, right, on = \"MaskID\",how = \"outer\"), \\\n",
    "                 [generate_split_frame_test_type(dataframe[dataframe[\"TEST_NAME\"] == name], \\\n",
    "                                    \"MaskID\", \"TEST_NAME\", name) for name in distinct_test_types])\n",
    "\n",
    "def generate_split_frame_test_type(dataframe, key_column, split_column, split_value):\n",
    "    categorical_test_names = [\"CTLA1\", \"CTLA2\", \"DPA1_Allele1\", \"DBA1_Allele2\",\"DPB1_Allele1\" \"DPB1_Allele2\", \"DQA1\", \"DQA2\", \"DQB1\", \"DQB2\", \"DR1\", \"DR2\", \\\n",
    "                         \"DRB3_Allele1\", \"DRB3_Allele2\", \"DRB4_Allele1\", \"DRB4_Allele2\", \"DRB5_Allele1\", \"DRB5_Allele2\", \\\n",
    "                          \"INS1\", \"INS2\", \"PTPN221\", \"PTPN222\", \"HLA-B_Allele1\", \"HLA-B_Allele2\"]\n",
    "    if(split_value not in categorical_test_names):\n",
    "        dataframe.loc[:,\"RESULT\"] = pd.to_numeric(dataframe.loc[:,\"RESULT\"], errors = 'coerce')\n",
    "    split_value = re.sub(\"_\", '-', str(split_value))\n",
    "    dataframe = dataframe.drop(split_column, axis = 1)\n",
    "    column_list = list(dataframe.columns)\n",
    "    column_list.pop(0)\n",
    "    modified_column_list = [split_value + \"_\" + column \\\n",
    "                           for column in column_list]\n",
    "    dataframe.columns = [key_column] + modified_column_list\n",
    "    \n",
    "    case_list = np.unique(dataframe[\"MaskID\"])\n",
    "    if len(case_list) == len(dataframe):\n",
    "        return dataframe\n",
    "    return sort_and_merge_by_observation(dataframe, split_value + '_' + \"DRAW_AGE\")\n",
    "\n",
    "                                 \n",
    "def prepend_file_name(file, column_list):\n",
    "    file_name = re.sub(\".*/(.*)\\.csv\", r\"\\g<1>\", file, flags = re.IGNORECASE)\n",
    "\n",
    "    #Add file name to every column aside from the MaskID\n",
    "    column_list[1:len(column_list)] = [file_name + '_' + column for column in column_list[1:len(column_list)]]\n",
    "    return column_list\n",
    "\n",
    "def retrieve_timepoint_variable(column_list):\n",
    "    if \"EVENT_AGE\" in column_list:\n",
    "        return \"EVENT_AGE\"\n",
    "    elif \"EFFECTIVE_AGE\" in column_list:\n",
    "        return \"EFFECTIVE_AGE\"\n",
    "    elif \"visit\" in column_list:\n",
    "        return \"visit\"\n",
    "    elif \"Evaluate_age\" in column_list:\n",
    "        return \"Evaluate_Age\"\n",
    "    return False\n",
    "\n",
    "def retrieve_sample_variable(column_list):\n",
    "    if \"sample_mask_id\" in column_list:\n",
    "        return \"sample_mask_id\"\n",
    "    elif \"SampleMaskID\" in column_list:\n",
    "        return \"SampleMaskID\"\n",
    "    return False\n",
    "\n",
    "def read_teddy_file_create_observations(file):\n",
    "    base_frame = pd.read_csv(file, low_memory = False)\n",
    "    \n",
    "    #make MaskID a string\n",
    "    base_frame[\"MaskID\"] = base_frame[\"MaskID\"].astype(str)\n",
    "    \n",
    "    #if the file is the \"TEST_RESULTS\" file, it needs to be processed differently\n",
    "    if re.search(\"TEST_RESULTS\", file) != None:\n",
    "        return_frame = split_and_merge_by_test_type(base_frame)\n",
    "        return_frame.columns = prepend_file_name(file, list(return_frame.columns))\n",
    "        return return_frame\n",
    "    \n",
    "    #remove columns where 95+% of values are missing\n",
    "    base_frame = remove_columns_low_representation(base_frame, .05)\n",
    "    \n",
    "    column_list = list(base_frame.columns)\n",
    "    \n",
    "    #determine if there are duplciate cases\n",
    "    cases = base_frame[\"MaskID\"]\n",
    "    row_count = len(base_frame)\n",
    "    number_unique_cases = len(np.unique(cases))\n",
    "    duplicate_cases = row_count != number_unique_cases\n",
    "    \n",
    "    #if there are no duplicates, return the frame\n",
    "    if not duplicate_cases:\n",
    "        base_frame.columns = prepend_file_name(file, column_list)\n",
    "        return base_frame\n",
    "    \n",
    "    #if there are, identify a column that defines the observation\n",
    "    #SampleMaskID is treated like a timepoint as all IDs are integers.\n",
    "    timepoint_column = retrieve_timepoint_variable(column_list)\n",
    "    sample_column = retrieve_sample_variable(column_list)\n",
    "    \n",
    "    \n",
    "    if (sample_column and timepoint_column) or ((not timepoint_column) and (not sample_column)):\n",
    "        return_frame = sort_and_merge_by_observation(base_frame)\n",
    "     \n",
    "    if sample_column:\n",
    "        #placeholder in case there are other columns that can be used for sorting\n",
    "        return_frame = sort_and_merge_by_observation(base_frame, sample_column)\n",
    "        \n",
    "    elif timepoint_column:\n",
    "        return_frame = sort_and_merge_by_observation(base_frame, timepoint_column)\n",
    "\n",
    "    #add the file name to the beginning of each column name\n",
    "    return_frame.columns = prepend_file_name(file, list(return_frame.columns))\n",
    "    return return_frame\n",
    "\n",
    "def get_timepoint(dataframe_columns, file):\n",
    "    #first, filter the columns by the file\n",
    "    pattern = (\"^\" + file + \"_\")\n",
    "    pattern_filter = [(re.search(pattern, column) != None) for column in dataframe_columns]\n",
    "    pattern_filter = [int(x) for x in pattern_filter]\n",
    "    dataframe_columns = list(compress(dataframe_columns, pattern_filter))\n",
    "    \n",
    "    #look for which column is present in the column list, with a preference for \"EVENT_AGE\", and so on\n",
    "    if sum([re.search(\"EVENT_AGE\", column) != None for column in dataframe_columns]) > 0:\n",
    "        return \"EVENT_AGE\"\n",
    "    elif sum([re.search(\"visit\", column) != None for column in dataframe_columns]) > 0:\n",
    "        return \"visit\"\n",
    "    elif sum([re.search(\"EFFECTIVE_AGE\", column) != None for column in dataframe_columns]) > 0:\n",
    "        return \"EFFECTIVE_AGE\"\n",
    "    elif sum([re.search(\"DRAW_AGE\", column) != None for column in dataframe_columns]) > 0:\n",
    "        return \"DRAW_AGE\"\n",
    "    return None\n",
    "\n",
    "#function for getting the mode disregaring missing values. The first value is taken in case of a tie\n",
    "def get_mode(value_list):\n",
    "    value_list = list(compress(value_list, [np.invert(missing_value(val)) for val in value_list]))\n",
    "    return mode(value_list)\n",
    "\n",
    "#function for flattening nested lists into a single list\n",
    "def flatten(list_of_lists):\n",
    "    if not isinstance(list_of_lists, list):\n",
    "        yield list_of_lists\n",
    "    else:\n",
    "        for entry in list_of_lists:\n",
    "            yield from flatten(entry)\n",
    "\n",
    "#calculates the boundries for the bins\n",
    "def divide_by_bins(start, end, bins):\n",
    "    time_range = end - start\n",
    "    bin_length = time_range/bins\n",
    "    return [start + x * bin_length for x in range(0, bins + 1)]\n",
    "\n",
    "#assigns a bin based on the list of boundries. NA timepoints are assigned the first bin\n",
    "def assign_bin(value_list, bin_list):\n",
    "    bin_value_list = []\n",
    "    for value in value_list:\n",
    "        for i in range(0, len(bin_list) - 1):\n",
    "            if value != value or value <= bin_list[i + 1]:\n",
    "                bin_value_list.append(i + 1)\n",
    "                break\n",
    "    return bin_value_list\n",
    "\n",
    "#function for determining which bin (which has data) is closest to the current bin\n",
    "def closest_bin_with_data(bin_, bin_timepoint, bins_with_data, bin_timepoint_dict):\n",
    "    lowest_bin_number = None\n",
    "    current_distance = None\n",
    "    for bin_num in bins_with_data:\n",
    "        if len(bin_timepoint_dict[bin_num]) == 0 or sum([missing_value(val) for val in bin_timepoint_dict[bin_num]]) == len(bin_timepoint_dict[bin_num]):\n",
    "            continue\n",
    "        distance = abs(np.nanmean(bin_timepoint_dict[bin_num]) - bin_timepoint)\n",
    "        if current_distance == None or current_distance > distance:\n",
    "            current_distance = distance\n",
    "            lowest_bin_number = bin_num\n",
    "    return lowest_bin_number\n",
    "\n",
    "def gather_observations_and_bin(dataframe, timepoint_column, max_bins, name):\n",
    "    ### Input: A Dataframe, the timepoint, the manually-set max # of bins and the file name of the original study\n",
    "    ### Output: A Dataframe containing the binned_columns to be merged to the original frame\n",
    "    \n",
    "    #add the \"_\" to the file name\n",
    "    name = name + \"_\"\n",
    "    \n",
    "    #remove the file name from the column list\n",
    "    column_list = list(dataframe.columns)\n",
    "    column_list = [re.sub(name, '', column) for column in column_list]\n",
    "    dataframe.columns = column_list\n",
    "    #isolate the columns without observations this indicates a subsection of the study with one observation per case\n",
    "    columns_no_observations = list(compress(column_list, \\\n",
    "                                           [re.search(\"observation_[0-9]+_\", column) == None for column in column_list]))\n",
    "    \n",
    "    column_list = list(compress(column_list, \\\n",
    "                               [re.search(\"observation_[0-9]+_\", column) != None for column in column_list]))\n",
    "    \n",
    "    \n",
    "    \n",
    "    #remove MaskID from the column list, as we will not manipulate that\n",
    "    #column_list = column_list[1:len(column_list)]\n",
    "    \n",
    "    #get the unique observation #s for this particular study\n",
    "    observation_list = np.unique([re.search(\"observation_[0-9]+_\", column)[0] for column in column_list])\n",
    "    \n",
    "    #get the list of cases for this study\n",
    "    case_list = dataframe[\"MaskID\"]\n",
    "    \n",
    "    #get the list of column types; the type of column observation_3_a would be a\n",
    "    column_type_list = np.unique([re.sub(\"observation_[0-9]+_\", '', column) for column in column_list])\n",
    "    \n",
    "    \n",
    "        #get a flat list of timepoints across all observations\n",
    "    timepoint_set = list(compress(column_list, \\\n",
    "                                    [re.search(timepoint_column, column) != None for column in column_list]))\n",
    "    \n",
    "    timepoint_lists = [(list(dataframe.loc[:,timepoint_set].iloc[i])) for i in range(0, len(dataframe))]\n",
    "    timepoint_list = list(flatten(timepoint_lists))\n",
    "    if name == \"TEST_RESULTS_\":\n",
    "        number_of_bins = max_bins\n",
    "    else:\n",
    "        #create a list of the number of observations per row\n",
    "        observation_completeness_list = [len(observation_list) - sum(pd.isna(list(dataframe.loc[:,timepoint_set].iloc[i]))) for i in range(0, len(dataframe))]\n",
    "        observation_completeness_list = list(compress(observation_completeness_list, [completeness != 0  for completeness in observation_completeness_list]))\n",
    "        #set the number of bins to either the median number of observations per row with NAs removed, or the user set max\n",
    "        number_of_bins = min(max_bins, int(np.nanmedian(observation_completeness_list)))\n",
    "    \n",
    "    #create the timepoint dividers. There will be # of bins _ 1 dividers, with the earliest timepoint representing the \"start\"\n",
    "    #every timepoint thereafter will be i * (latest timepoint - earliest timepoint)/i\n",
    "    bin_list = divide_by_bins(np.nanmin(timepoint_list), np.nanmax(timepoint_list), number_of_bins)\n",
    "    \n",
    "    bin_numbers = range(1, number_of_bins + 1)\n",
    "    bin_strings = [\"bin_\" + str(bin_number) for bin_number in bin_numbers]\n",
    "    \n",
    "    #create a dictionary of the mean of the border for each timepoint to reference later\n",
    "    bin_time_dict = {}\n",
    "    for i in range(0, len(bin_list) - 1):\n",
    "        bin_time_dict[\"bin_\" + str(i + 1)] = np.nanmean([bin_list[i], bin_list[i + 1]])\n",
    "    \n",
    "    #create a dictionary of various properties of each colum type to reference later\n",
    "    column_type_dict = {}\n",
    "    \n",
    " \n",
    "    for column_type in column_type_list:\n",
    "        subdict = {}\n",
    "        subdict[\"column_subset\"] = list(compress(column_list, \\\n",
    "                                         [re.search((column_type + \"$\"), column) != None for column in column_list]))\n",
    "        subdict[\"observation_subset\"] = np.unique([re.search(\"observation_[0-9]+_\", column)[0] for column in subdict[\"column_subset\"]])\n",
    "        subdict[\"timepoint_subset\"] = list(compress(subdict[\"column_subset\"], \\\n",
    "                                            [(re.search(timepoint_column, column) != None) for column in subdict[\"column_subset\"]]))\n",
    "        column_type_dict[column_type] = subdict\n",
    "        \n",
    "    #create an empty list to store the dicts generated below\n",
    "    dict_list = []\n",
    "    #for every row\n",
    "    for i in range(0, len(dataframe)):\n",
    "        #get the MaskID\n",
    "        newdf={\"MaskID\": case_list[i]}\n",
    "        for column_type in column_type_list:\n",
    "            #get the subset of columns that match that type\n",
    "            column_subset = column_type_dict[column_type][\"column_subset\"]\n",
    "            timepoint_subset = column_type_dict[column_type][\"timepoint_subset\"]\n",
    "            #make a list of the data\n",
    "            data_list = list(dataframe.loc[:,column_subset].iloc[i])\n",
    "            #make a list of the timepoints\n",
    "            timepoint_list = list(dataframe.loc[:,timepoint_subset].iloc[i])\n",
    "            \n",
    "            #assign bins for each timepoint. Add the data and timepoint to their respective bin\n",
    "            binned_timepoints = assign_bin(timepoint_list, bin_list)\n",
    "            bin_dict = dict(zip(bin_strings, [[] for j in range(0, len(bin_list))]))\n",
    "            \n",
    "            bin_timepoints = dict(zip(bin_strings, [[] for j in range(0, len(bin_list))]))\n",
    "            for x in range(0, len(binned_timepoints)):\n",
    "                bin_ = \"bin_\" + str(binned_timepoints[x])\n",
    "                bin_dict[bin_].append(data_list[x])\n",
    "                bin_timepoints[bin_].append(timepoint_list[x])\n",
    "                \n",
    "            #find which bins do not have data\n",
    "            empty_bins = list(compress(bin_strings, \\\n",
    "                                            [len(bin_dict[bin_]) == 0 or \\\n",
    "                                             sum([(missing_value(value)) for value in bin_dict[bin_]]) == len(bin_dict[bin_]) for bin_ in bin_strings]))\n",
    "            #find which bins have data\n",
    "            bins_with_data = list(compress(bin_strings, [np.invert(bin_ in empty_bins) for bin_ in bin_strings]))\n",
    "            \n",
    "            #for the bins which do not have data, find the closest bin by abs(midpoint_of_bin - average_of_other_bin_timepoints)\n",
    "            for bin_ in empty_bins:\n",
    "                closest_bin_value = closest_bin_with_data(bin_, bin_time_dict[bin_], bins_with_data, bin_timepoints)\n",
    "                if closest_bin_value != None:\n",
    "                    bin_dict[bin_] = bin_dict[closest_bin_value]\n",
    "            \n",
    "            #for each bin, add the element to the dictionary\n",
    "            for bin_ in bin_strings:\n",
    "                new_column = bin_ +\"_\" + column_type\n",
    "                bin_data = bin_dict[bin_]\n",
    "                \n",
    "                if len(bin_data) == 0 or sum(value != value for value in bin_data) == len(bin_data):\n",
    "                    continue\n",
    "                else:\n",
    "                    try:\n",
    "                        newdf[new_column] = np.nanmean(bin_data)\n",
    "                    except TypeError:\n",
    "                        newdf[new_column] = get_mode(bin_data)\n",
    "        #append the dictionary to the main list\n",
    "        dict_list.append(newdf)\n",
    "    \n",
    "    #add the file name back to the beginning of each column, aside from the \"MaskID\"\n",
    "    return_frame = pd.DataFrame(dict_list)\n",
    "    return_frame = pd.merge(dataframe.loc[:,columns_no_observations], return_frame, on = \"MaskID\", how = \"outer\")\n",
    "    return_frame_columns = list(return_frame.columns)\n",
    "    return_frame_columns = return_frame_columns[1:len(return_frame_columns)]\n",
    "    return_frame_columns = [name + \"_\" + column for column in return_frame_columns]\n",
    "    return_frame_columns.insert(0, 'MaskID')\n",
    "    return_frame.columns = return_frame_columns\n",
    "    return return_frame\n",
    "\n",
    "def remove_columns_with_all_missing(data_frame):\n",
    "    \"\"\"\n",
    "    Removes columns in the given DataFrame where all values are missing.\n",
    " \n",
    "    Parameters:\n",
    "    - data_frame (pd.DataFrame): The input DataFrame.\n",
    " \n",
    "    Returns:\n",
    "    - pd.DataFrame: A new DataFrame with columns containing all missing values removed.\n",
    "    \"\"\"\n",
    "    # Find columns with all missing values\n",
    "    columns_with_all_missing = data_frame.columns[data_frame.isnull().all()].tolist()\n",
    " \n",
    "    # Drop columns with all missing values\n",
    "    new_data_frame = data_frame.drop(columns=columns_with_all_missing)\n",
    " \n",
    "    return new_data_frame\n",
    "\n",
    "def summary_stat(value_list):\n",
    "    if len(value_list) == 0:\n",
    "        return np.nan\n",
    "    if sum([missing_value(val) for val in value_list]) == len(value_list):\n",
    "        return np.nan\n",
    "    try:\n",
    "        return_value = np.nanmean(value_list)\n",
    "    except TypeError:\n",
    "        return_value = get_mode(value_list)\n",
    "    return return_value\n",
    "\n",
    "def summarize_observations(dataframe, name):\n",
    "    #add the \"_\" to the file name\n",
    "    name = name + \"_\"\n",
    "    \n",
    "    #remove the file name from the column list\n",
    "    column_list = list(dataframe.columns)\n",
    "    column_list = [re.sub(name, '', column) for column in column_list]\n",
    "    dataframe.columns = column_list\n",
    "    #remove MaskID from the column list, as we will not manipulate that\n",
    "    \n",
    "    column_list = column_list[1:len(column_list)]\n",
    "    column_list = list(dataframe.columns)\n",
    "    column_list = column_list[1:len(column_list)]\n",
    "    \n",
    "    column_type_list = np.unique([re.sub('observation_[0-9]+_', '', column) for column in column_list])\n",
    "    return_frame = pd.DataFrame(data = dataframe[\"MaskID\"], columns = [\"MaskID\"])\n",
    "    for column_type in column_type_list:\n",
    "        column_set = list(compress(column_list, \\\n",
    "                                  [re.search(column_type, column) != None for column in column_list]))\n",
    "        return_frame[column_type] = dataframe[column_set].apply(summary_stat, 1)\n",
    "    \n",
    "    return_frame_columns = list(return_frame.columns)\n",
    "    return_frame_columns = return_frame_columns[1:len(return_frame_columns)]\n",
    "    return_frame_columns = [name + column for column in return_frame_columns]\n",
    "    return_frame_columns.insert(0, 'MaskID')\n",
    "    #drop sample_mask_id if it exists\n",
    "    sample_id_columns = list(compress(return_frame_columns, [re.search(\"sample_mask_id\", column, re.IGNORECASE) != None for column in return_frame_columns]))\n",
    "    \n",
    "    return_frame.columns = return_frame_columns\n",
    "    return_frame.drop(sample_id_columns, axis = 1, inplace = True)\n",
    "    return return_frame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45e60121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(817, 71928)\n",
      "The running time of generating the raw merged dataset is: 120.78s\n",
      "processing: ANNUAL_CHILD_QUESTIONNAIRE on: EVENT_AGE\n",
      "processing: ANNUAL_QUESTIONNAIRE on: EVENT_AGE\n",
      "No timepoint found in: ASCORBIC_ACID_NIDDK\n",
      "No timepoint found in: CAROTENOIDS_NIDDK\n",
      "processing: CBCL on: EVENT_AGE\n",
      "processing: CELIAC_DISEASE_DIAGNOSIS_FORM on: EVENT_AGE\n",
      "processing: CHANGE_IN_STUDY_PARTICIPATION on: EFFECTIVE_AGE\n",
      "No timepoint found in: CHOLESTEROL_NIDDK\n",
      "processing: DIABETES_MANAGEMENT on: EVENT_AGE\n",
      "processing: FAMILY_HISTORY on: EVENT_AGE\n",
      "No timepoint found in: FAMILY_RELATIVE\n",
      "No timepoint found in: FATTY_ACIDS_NIDDK\n",
      "processing: GLUTEN_FREE_DIET_UPDATE on: EVENT_AGE\n",
      "processing: LAST_QUESTIONNAIRE on: EVENT_AGE\n",
      "processing: MMTT_PROCEDURE_FORM on: EVENT_AGE\n",
      "processing: NINE_MONTH_PARENT_QUESTIONNAIRE on: EVENT_AGE\n",
      "processing: NON_TEDDY_RESEARCH_FORM on: EVENT_AGE\n",
      "No timepoint found in: OLINK_INFLAMMATION_NIDDK\n",
      "processing: PARENT_PEDSQL_5_7 on: EVENT_AGE\n",
      "processing: PARENT_PEDSQL_8_12 on: EVENT_AGE\n",
      "processing: PED_INVENTORY_PARENTS on: EVENT_AGE\n",
      "processing: PHYSICAL_EXAM on: EVENT_AGE\n",
      "processing: PUBERTAL_ASSESSMENT_FEMALE on: EVENT_AGE\n",
      "processing: PUBERTAL_ASSESSMENT_MALE on: EVENT_AGE\n",
      "No timepoint found in: RETINOL_NIDDK\n",
      "processing: SIX_MONTH_PARENT_QUESTIONNAIRE on: EVENT_AGE\n",
      "processing: STAI_PARENTS on: EVENT_AGE\n",
      "processing: SYMPTOMS_OF_CELIAC_DISEASE on: EVENT_AGE\n",
      "processing: TEDDYBOOK on: EVENT_AGE\n",
      "processing: TEDDY_UPDATE_FORM on: EVENT_AGE\n",
      "processing: TEST_RESULTS on: DRAW_AGE\n",
      "No timepoint found in: TOCOPHEROLS_NIDDK\n",
      "No timepoint found in: VITD_NIDDK\n",
      "processing: activity_monitor on: visit\n",
      "processing: adverse_event_table on: EVENT_AGE\n",
      "processing: teddybook2_5 on: EVENT_AGE\n",
      "processing: teddybook6_12 on: EVENT_AGE\n",
      "(817, 1433)\n",
      "The running time of generating the binned merged dataset is: 1456.86s\n"
     ]
    }
   ],
   "source": [
    "#file_directory = \"DATA/TEDDY/\"\n",
    "file_directory = \"/home/NIDDK/DATA/TEDDY/\"\n",
    "file_list = os.listdir(file_directory)\n",
    "\n",
    "#comment/remove the line below to include these files. Will take a very long time\n",
    "#exclusion_list = [\"TEST_RESULTS.csv\", \"FAMILY_HISTORY.CSV\", \"teddybook2_5.csv\",\"teddybook6_12.csv\", \"TEDDYBOOK.CSV\"]\n",
    "#file_exclusion_mask = np.invert([(file in exclusion_list) for file in file_list])\n",
    "#file_list = [\"TEST_RESULTS.csv\"]\n",
    "#file_list = list(compress(file_list, file_exclusion_mask))\n",
    "time1 = time.time()\n",
    "file_list = [file_directory + file for file in file_list]\n",
    "file_list = file_list\n",
    "file_list_mask = [\".csv\" in file.lower() for file in file_list]# for catching non-csv files\n",
    "file_list = list(compress(file_list, file_list_mask))\n",
    "\n",
    "dataframe_list = [read_teddy_file_create_observations(file) for file in file_list]\n",
    "dataframe_mask = [len(x) > 0 for x in dataframe_list] # for catching empty dataframes\n",
    "dataframe_list = list(compress(dataframe_list, dataframe_mask))\n",
    "\n",
    "merged_data = reduce(lambda left, right: pd.merge(left, right, on = \"MaskID\",how = \"outer\"), dataframe_list)\n",
    "merged_data.to_csv(\"Raw-Dataset.csv\", index = False)\n",
    "print(merged_data.shape)\n",
    "print(\"The number of the numeric features of the raw data set is: %d\"%(len(merged_data.select_dtypes(include='number').columns)))\n",
    "print(\"The running time of generating the raw merged dataset is: %.2fs\"%(time.time()-time1))\n",
    "\n",
    "time2 = time.time()\n",
    "filtered_dataframe = remove_columns_with_all_missing(merged_data)\n",
    "\n",
    "merged_column_list = list(filtered_dataframe.columns)\n",
    "columns_with_observations = list(compress(merged_column_list, [re.search(\"_observation_\", column) for column in merged_column_list]))\n",
    "files_with_observations = np.unique([re.search(\"(.*)_observation.*\", column)[1] for column in columns_with_observations])\n",
    "files_with_observations\n",
    "\n",
    "filtered_and_binned_dataframe = filtered_dataframe\n",
    "files_with_no_timepoints = []\n",
    "for file in files_with_observations:\n",
    "    timepoint_columns = get_timepoint(filtered_and_binned_dataframe, file)\n",
    "    if timepoint_columns == None:\n",
    "        print(\"No timepoint found in: \" + file)\n",
    "        files_with_no_timepoints.append(file)\n",
    "        continue\n",
    "    print(\"processing: \" + file +\" on: \" + timepoint_columns)\n",
    "    filtered_columns = list(compress(filtered_and_binned_dataframe.columns, [re.search(\"^\" + file + \"_\", column) != None for column in filtered_and_binned_dataframe.columns] ))\n",
    "    filtered_columns.insert(0, \"MaskID\")\n",
    "    binned_frame = gather_observations_and_bin(filtered_and_binned_dataframe.loc[:,filtered_columns].copy(), timepoint_columns, 3, file)\n",
    "    #drop the original columns\n",
    "    columns_to_drop = filtered_columns[1:len(filtered_columns)]\n",
    "    filtered_and_binned_dataframe.drop(columns_to_drop, axis = 1, inplace = True)\n",
    "    filtered_and_binned_dataframe = pd.merge(filtered_and_binned_dataframe, binned_frame, on = \"MaskID\", how = \"outer\")\n",
    "\n",
    "for file in files_with_no_timepoints:\n",
    "    filtered_columns = list(compress(filtered_and_binned_dataframe.columns, [re.search(\"^\" + file + \"_\", column) != None for column in filtered_and_binned_dataframe.columns]))\n",
    "    filtered_columns.insert(0, \"MaskID\")\n",
    "    summarized_data = summarize_observations(filtered_and_binned_dataframe.loc[:, filtered_columns], file)\n",
    "    columns_to_drop = filtered_columns[1:len(filtered_columns)]\n",
    "    filtered_and_binned_dataframe.drop(columns_to_drop, axis = 1, inplace = True)\n",
    "    filtered_and_binned_dataframe = pd.merge(filtered_and_binned_dataframe, summarized_data, on = \"MaskID\", how = \"outer\")\n",
    "\n",
    "filtered_and_binned_dataframe.to_csv(\"Binned_Dataframe.csv\", index = False)\n",
    "print(filtered_and_binned_dataframe.shape)\n",
    "print(\"The running time of generating the binned merged dataset is: %.2fs\"%(time.time()-time2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8358cdc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Process or clean the strings (texts) in the data frame, including convert strings to lowercase and lemmatization.\n",
    "# Input: data frame generated in Step 1\n",
    "# Output: data frame\n",
    "def preprocess_text(text):\n",
    "    # detect language\n",
    "    #lang = detect(text.lower())\n",
    "\n",
    "    # Tokenize\n",
    "    #try:\n",
    "    #    tokens = word_tokenize(text)\n",
    "    #except TypeError:\n",
    "    #    try:\n",
    "    #        text = text.decode()\n",
    "    #    except AttributeError:\n",
    "    #        print(text)\n",
    "    #        print(type(text))\n",
    "    #    tokens = word_tokenize(text)\n",
    "    try:\n",
    "        tokens = word_tokenize(text)\n",
    "    except:\n",
    "        text = str(text)\n",
    "        tokens = word_tokenize(text)\n",
    "    #tokens = word_tokenize(text)\n",
    "    # Remove punctuation and convert to lowercase\n",
    "    tokens = [word.lower() for word in tokens if word.isalpha()]\n",
    "    \n",
    "    #if lang != 'en':\n",
    "    #    tokens = [translator.translate(word, dest='en') for word in tokens]\n",
    "    \n",
    "    # Remove stop words\n",
    "    #stop_words = set(stopwords.words('english'))\n",
    "    #tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    \n",
    "\n",
    "    # Check if the list of tokens is not empty\n",
    "    if tokens:\n",
    "        # Reassemble the text\n",
    "        cleaned_text = ' '.join(tokens)\n",
    "        return cleaned_text\n",
    "    else:\n",
    "        # If the list is empty, return the original text\n",
    "        return text\n",
    "    \n",
    "    return cleaned_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee345564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(817, 1433)\n",
      "The running time of cleaning the text in dataset is: 25.13s\n"
     ]
    }
   ],
   "source": [
    "#cleaned_text_df = pd.read_csv('NoEmptyColumns-Dataset.csv', low_memory=False)\n",
    "time3 = time.time()\n",
    "cleaned_text_df = filtered_and_binned_dataframe.copy()\n",
    "for column in cleaned_text_df.select_dtypes(include=['object']).columns:\n",
    "    if column != 'MaskID':\n",
    "        cleaned_text = cleaned_text_df[column].fillna('not reported').apply(preprocess_text)\n",
    "        cleaned_text_df[column] = cleaned_text\n",
    "\n",
    "cleaned_text_df.to_csv('PreprocessedText_Dataset.csv', index=False)\n",
    "print(cleaned_text_df.shape)\n",
    "print(\"The running time of cleaning the text in dataset is: %.2fs\"%(time.time()-time3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "714ccdf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3.1: Impute the missing values in the numeric columns with the completeness is larger than the threshold using Multiple Imputation by Chained Equation algorithm (MICE)\n",
    "# Input: data frame with only numeri columns generated in Step 2\n",
    "# Output: data frame\n",
    "def completeness_criteria(column, threshold):\n",
    "    non_null_percentage = column.count() / len(column)\n",
    "    return non_null_percentage > threshold\n",
    "\n",
    "def filter_df_by_completeness(df, completeness_threshold):\n",
    "    # Apply the custom criteria function to each column\n",
    "    selected_columns = df.apply(completeness_criteria, threshold=completeness_threshold)\n",
    "    # Extract the column names that meet the completeness criteria\n",
    "    selected_columns = selected_columns[selected_columns].index\n",
    "    return df[selected_columns]\n",
    "    \n",
    "\n",
    "def impute_missing_numeric_values(df, ignored_categorical_columns, completeness_threshold):\n",
    "    numeric_cols_org = df.select_dtypes(include='number').columns\n",
    "    print(\"The number of the original numeric features from the binned dataframe is: %d\"%(len(numeric_cols_org)))\n",
    "    df_filtered = filter_df_by_completeness(df, completeness_threshold)\n",
    "    print(\"data frame is filtered by the completness.\")\n",
    "    impute_data = df_filtered.copy()\n",
    "    impute_data_v2_tmp = df_filtered.copy()\n",
    "    \n",
    "    # Separate numeric and categorical columns\n",
    "    numeric_cols = impute_data.select_dtypes(include='number').columns\n",
    "    categorical_cols = impute_data.select_dtypes(exclude='number').columns\n",
    "    print(\"The number of the numeric features filtered by completeness threshold %.2f is: %d\"%(completeness_threshold, len(numeric_cols)))\n",
    "    #print(len(numeric_cols))\n",
    "    #print(numeric_cols)\n",
    "    # Impute missing values in numeric columns using IterativeImputer with default estimator\n",
    "    numeric_imputer = IterativeImputer(max_iter=10, random_state=100)\n",
    "    # Train the imputor model\n",
    "    numeric_imputer.fit(impute_data[numeric_cols])\n",
    "    # Predict the missing values. This is done using the `transform` method.\n",
    "    impute_data[numeric_cols] = numeric_imputer.transform(impute_data[numeric_cols])\n",
    "    impute_data_v2_tmp[numeric_cols] = impute_data_v2_tmp[numeric_cols].fillna(\"Imputed\")\n",
    "    impute_data_v2 = impute_data_v2_tmp[numeric_cols]\n",
    "    impute_data_v2.insert(0, \"MaskID\", list(impute_data_v2_tmp.loc[:,\"MaskID\"]))\n",
    "    #impute_data_v2 = impute_data_v2_tmp\n",
    "    return (impute_data, impute_data_v2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4eab40dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of the original numeric features from the binned dataframe is: 936\n",
      "data frame is filtered by the completness.\n",
      "The number of the numeric features filtered by completeness threshold 0.40 is: 299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/sklearn/impute/_iterative.py:800: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_17351/1138494026.py:38: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  impute_data_v2.insert(0, \"MaskID\", list(impute_data_v2_tmp.loc[:,\"MaskID\"]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(817, 796)\n",
      "The running time of imputing missing numeric values is: 141.48s\n"
     ]
    }
   ],
   "source": [
    "Completeness_threshold = 0.4\n",
    "ignored_categorical_columns = [\"MaskID\"]\n",
    "time4 = time.time()\n",
    "merged_data_imputed, merged_data_imputed_labeled = impute_missing_numeric_values(cleaned_text_df, ignored_categorical_columns, Completeness_threshold)\n",
    "merged_data_imputed.to_csv('Imputed_Dataset.csv', index=False)\n",
    "merged_data_imputed_labeled.to_csv('Imputed_Labeled_Dataset.csv', index=False)\n",
    "print(merged_data_imputed.shape)\n",
    "print(\"The running time of imputing missing numeric values is: %.2fs\"%(time.time()-time4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "227db3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3.2: Encode the categorical columns in the data frame\n",
    "# Input: data frame with only categorical columns generated in Step 2\n",
    "# Output: data frame\n",
    "def one_hot_encode_dataframe(data_frame):\n",
    "    \"\"\"\n",
    "    Objective: Performs one-hot encoding on categorical columns in the dataframe\n",
    "    \n",
    "    Parameters:\n",
    "        - data_frame (pd.DataFrame): Input dataframe\n",
    " \n",
    "    Returns:\n",
    "        - pd.DataFrame: A new DataFrame with one-hot encoded categorical columns and original numeric columns\n",
    "    \"\"\"\n",
    "   \n",
    "    #Save the first column then drop it from the dataframe to avoid encoding\n",
    "    first_column = data_frame.iloc[:, 0]\n",
    "    data_frame = data_frame.iloc[: , 1:]\n",
    "    \n",
    "    #Filter categorical columns within the dataframe\n",
    "    categorical_cols_df = data_frame.select_dtypes(include=['object']).copy()\n",
    "    \n",
    "    #Fill-in missing values in categorical columns\n",
    "    categorical_cols_df = categorical_cols_df.fillna('not reported')\n",
    "    \n",
    "    #Initialize OneHotEncoding\n",
    "    one_hot_encoding = OneHotEncoder(sparse_output=False, dtype=np.int64)\n",
    "    \n",
    "    #Dictionary to store mapping of encoded vallues\n",
    "    #ohe_dict = {}\n",
    "\n",
    "    #Apply one hot encoding to all categorical columns\n",
    "    for col in categorical_cols_df.columns:\n",
    "        ohe_data = pd.DataFrame(one_hot_encoding.fit_transform(categorical_cols_df[categorical_cols_df.columns]),\n",
    "                            columns= one_hot_encoding.get_feature_names_out(categorical_cols_df.columns))\n",
    "        #ohe_dict[col] = dict(zip(  ,one_hot_encoding.categories_))\n",
    "  \n",
    "    #Concat original numerical columns to one-hot encoded columns only\n",
    "    ohe_data = pd.concat([ohe_data, data_frame.drop(columns= categorical_cols_df.columns)], axis=1)\n",
    "    \n",
    "    #Place ID column back in the dataframe\n",
    "    ohe_data.insert(0, 'MaskID', first_column)\n",
    "    \n",
    "    return ohe_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cc8a0253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(817, 6832)\n",
      "The running time one hot encoding is: 106.74s\n"
     ]
    }
   ],
   "source": [
    "time5 = time.time()\n",
    "one_hot_encoded_data = one_hot_encode_dataframe(merged_data_imputed)\n",
    "print(one_hot_encoded_data.shape)\n",
    "print(\"The running time one hot encoding is: %.2fs\"%(time.time()-time5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a1469d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Write a csv file from the data frame genreated in Step 3.2\n",
    "one_hot_encoded_data.to_csv('TEDDY_data_final.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
