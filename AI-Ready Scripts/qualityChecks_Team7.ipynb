{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "441122c5",
   "metadata": {},
   "source": [
    "*__Note:__* Filepaths should be changed as necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a56b491",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3348fe28",
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "print(cwd)\n",
    "studiesDirPath = \"/home/NIDDK/DATA/TEDDY/\"\n",
    "dataDictionaryJSON = cwd + \"/TEDDY_data_dictionary_v1.2.json\";\n",
    "studyCSVFiles = (file for file in os.listdir(studiesDirPath) \n",
    "         if os.path.isfile(os.path.join(studiesDirPath, file)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4632040f",
   "metadata": {},
   "outputs": [],
   "source": [
    "studyFileNamesMap = {}\n",
    "for filename in studyCSVFiles:\n",
    "    studyFile = Path(filename).stem\n",
    "    studyFileNameLowerCase = studyFile.lower()\n",
    "    studyFileNamesMap[studyFileNameLowerCase] = filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2dca761",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for data stype and max length anomolies\n",
    "studyFileColumnMap = {}     \n",
    "list_of_column_names = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199d38c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read Data Dictionary\n",
    "with open(dataDictionaryJSON) as json_data:\n",
    "    dataDictionary = json.load(json_data)\n",
    "    allTypes = set()\n",
    "    for eachEntityrow in dataDictionary:\n",
    "        if (not eachEntityrow.startswith(\"teddy\") and not eachEntityrow.startswith(\"test_results\")):\n",
    "        #print(\"Processing entity: \" + eachEntityrow)\n",
    "            dictionaryValues = dataDictionary[eachEntityrow]\n",
    "        if (studyFileNamesMap.get(eachEntityrow)):\n",
    "            studyFileName = studyFileNamesMap.get(eachEntityrow)\n",
    "            dictColumnNames = list()\n",
    "            for issue in dictionaryValues:\n",
    "                dictColumnNames.append(issue['Variable'].lower())\n",
    "            #Parse the corresponding data file\n",
    "            with open(studiesDirPath + studyFileName, mode='r', encoding='utf-8') as f:\n",
    "                reader = csv.DictReader(f, delimiter=',')\n",
    "                columnNamesInStudyFile = list()\n",
    "                for col in reader.fieldnames:           \n",
    "                    columnNamesInStudyFile.append(col)\n",
    "                maxLengthExceededColumns = {}\n",
    "                typeCheckAnolomies = {}\n",
    "                noDataTypeDefinitions = {}\n",
    "                maxLengthExceededColumns[\"Table ~ Column ~ Row Num\"] = \"Value ~ Max Length from Dict ~ Value Length\"\n",
    "                typeCheckAnolomies[\"Table ~ Column ~ Row Num\"] = \"Value\"\n",
    "                noDataTypeDefinitions[\"Table ~ Column ~ Row Num\"] = \"Value\"\n",
    "                rowNum = 0\n",
    "                for row in reader:\n",
    "                    rowNum = rowNum + 1\n",
    "                    for columnName in columnNamesInStudyFile:\n",
    "                        columnValue = row[columnName]\n",
    "                        if (columnName.lower() in dictColumnNames):\n",
    "                            for issue in dictionaryValues:\n",
    "                                if (issue['Variable'].lower() == columnName.lower()):\n",
    "                                    dictCol = issue['Type']\n",
    "                                    maxLength = issue['Len']\n",
    "                                    if (dictCol == 'Num'):\n",
    "                                        if (columnValue != \"\" and maxLength != \"\"):\n",
    "                                            roundedVal = round(float(columnValue))\n",
    "                                            if (len(str(roundedVal)) > int(maxLength)):\n",
    "                                                if columnName not in maxLengthExceededColumns:\n",
    "                                                    maxLengthExceededColumns[eachEntityrow + \"~\" + columnName + \"~\" + str(rowNum)] = str(columnValue) + \" ~ \" +  str(maxLength) + \" ~ \" +  str(roundedVal)\n",
    "                                    else: \n",
    "                                        if (columnValue != \"\" and maxLength != \"\" and (len(str(columnValue)) > float(int(maxLength)))):\n",
    "                                            if columnName not in maxLengthExceededColumns:\n",
    "                                                maxLengthExceededColumns[eachEntityrow + \"~\" + columnName + \"~\" + str(rowNum)] = str(columnValue) + \" ~ \" +  str(maxLength) + \" ~ \" +  str(roundedVal)\n",
    "                                    if dictCol not in allTypes:\n",
    "                                        allTypes.add(dictCol)\n",
    "                                    if (dictCol == 'Num'):\n",
    "                                        numbercheck = columnValue.isnumeric()\n",
    "                                        decimalCheck = isinstance(columnValue, float)\n",
    "                                        if (not((numbercheck == True and decimalCheck == False) or (numbercheck == False or decimalCheck == True))):\n",
    "                                            if columnName not in typeCheckAnolomies:\n",
    "                                                typeCheckAnolomies[eachEntityrow + \"~\" + columnName + \"~\" + str(rowNum)] = columnValue\n",
    "                                    if (dictCol == 'Char'):\n",
    "                                        charCheck = isinstance(columnValue, str)\n",
    "                                        if (charCheck == False):\n",
    "                                            if columnName not in typeCheckAnolomies:\n",
    "                                                typeCheckAnolomies[eachEntityrow + \"~\" + columnName + \"~\" + str(rowNum)] = columnValue                               \n",
    "                if (len(maxLengthExceededColumns) > 1):\n",
    "                    df = pd.DataFrame(maxLengthExceededColumns, index=[0])\n",
    "                    print(\"Max Length Exceeded:\")\n",
    "                    print(df)\n",
    "                if (len(typeCheckAnolomies) > 1):\n",
    "                    df1 = pd.DataFrame(typeCheckAnolomies, index=[0])\n",
    "                    print(\"Data Type Anomalies:\")\n",
    "                    print(df1)\n",
    "                if (len(noDataTypeDefinitions) > 1):\n",
    "                    df2 = pd.DataFrame(noDataTypeDefinitions, index=[0])\n",
    "                    print(\"No data type defined for:\")\n",
    "                    print(df2) \n",
    "        else:\n",
    "            print(\"File not found:\" + eachEntityrow)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
